{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70503e15",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "\n",
    "## Dataset\n",
    "\n",
    "\n",
    "## Chosen Embedding models:\n",
    "\n",
    "- GloVe: Global Vectors for World Representation by Jeffrey Pennington,   Richard Socher,   Christopher D. Manning \n",
    "    - github: https://github.com/stanfordnlp/GloVe\n",
    "    - research paper:\n",
    "        - https://nlp.stanford.edu/pubs/glove.pdf\n",
    "        - https://nlp.stanford.edu/projects/glove/\n",
    "- OpenAI\n",
    "- SBert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d73be6",
   "metadata": {},
   "source": [
    "## 2.0 Implement Embedding Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb979c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.strip().replace('\\n', ' ')\n",
    "\n",
    "def sbert_embed(text):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return model.encode(text)\n",
    "\n",
    "def openai_embed(text):\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# def glove_embed(text):\n",
    "def glove_embed(text, glove_embeddings, vector_size=300):\n",
    "    words = text.split()\n",
    "    valid_vectors = []\n",
    "\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in glove_embeddings:\n",
    "            valid_vectors.append(glove_embeddings[word])\n",
    "\n",
    "    if valid_vectors:\n",
    "        return np.mean(valid_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3fa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collections=[CollectionDescription(name='sbert_embedding_collection'), CollectionDescription(name='my_collection'), CollectionDescription(name='glove_embedding_collection')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73594/53610660.py:4: UserWarning: Qdrant client version 1.14.2 is incompatible with server version 1.9.2. Major versions should match and minor version difference must not exceed 1. Set check_compatibility=False to skip version check.\n",
      "  client = QdrantClient(host=\"localhost\", port=6333)\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Connect to your Qdrant instance (adjust host/port if needed)\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# List all collections\n",
    "collections = client.get_collections()\n",
    "print(collections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783e330",
   "metadata": {},
   "source": [
    "### 3.0 Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e29b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    \"\"\"Splits long text into chunks of approximately max_tokens (words ≈ tokens).\"\"\"\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_tokens):\n",
    "        yield ' '.join(words[i:i + max_tokens])\n",
    "\n",
    "\n",
    "def load_pdfs_from_folder(folder_path):\n",
    "    pdf_texts = []\n",
    "    pdf_paths = Path(folder_path).glob(\"*.pdf\") \n",
    "    \n",
    "    for pdf_path in pdf_paths:\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"  # sometimes pages have no text\n",
    "        pdf_texts.append((str(pdf_path), text))\n",
    "    \n",
    "    return pdf_texts # returns tuples, filename, full_text\n",
    "\n",
    "# load embeddings\n",
    "def load_embeddings(glove_file_path):\n",
    "    embeddings = {}\n",
    "    with open(glove_file_path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = list(map(float, parts[1:]))\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def test_embedding(embedding_model):\n",
    "    pdfs = load_pdfs_from_folder(\"./data/v1/docs\") # 1. Load PDFs\n",
    "    embedded_pdfs = []\n",
    "\n",
    "    if (embedding_model==\"glove\"):\n",
    "        glove_path = \"glove.6B/glove.6B.300d.txt\" # 2. Load GloVe\n",
    "        glove_embeddings = load_embeddings(glove_path)\n",
    "    # 3. Embed each document\n",
    "        # here is where the actual embeddings are done (where glove_embed is called) and where they are added to a list of the embedded_pdfs through glove\n",
    "        for filename, text in pdfs:\n",
    "            for chunk in chunk_text(text, max_tokens=500):  # CHUNKING HERE\n",
    "                embedding = glove_embed(chunk, glove_embeddings, vector_size=300)\n",
    "                embedded_pdfs.append((filename, embedding, chunk))\n",
    "        return embedded_pdfs, glove_embeddings\n",
    "    \n",
    "    elif (embedding_model==\"sbert\"):\n",
    "        for filename, text in pdfs:\n",
    "            for chunk in chunk_text(text, max_tokens=500):\n",
    "                embedding = sbert_embed(chunk)\n",
    "                embedded_pdfs.append((filename, embedding, chunk))\n",
    "    \n",
    "    elif (embedding_model==\"open_ai\"):\n",
    "        for filename, text in pdfs:\n",
    "            for chunk in chunk_text(text, max_tokens=500):\n",
    "                embedding = openai_embed(chunk)\n",
    "                embedded_pdfs.append((filename, embedding, chunk))\n",
    "\n",
    "    else:\n",
    "        print(f\"embedding model {embedding_model} is not in this testing code\")\n",
    "\n",
    "    return embedded_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915af6b5",
   "metadata": {},
   "source": [
    "# Run each embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd32bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedded_pdfs, glove_embeddings = test_embedding(\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee8cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_embedded_pdfs = test_embedding(\"sbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedded_pdfs = test_embedding(\"open_ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d30e4c",
   "metadata": {},
   "source": [
    "# Save to Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6102600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_embeddings_to_jsonl(data, output_path=\"openai_embedded_pdfs.jsonl\"):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for filename, embedding, text in data:\n",
    "            record = {\n",
    "                \"filename\": filename,\n",
    "                \"embedding\": embedding.tolist() if isinstance(embedding, (np.ndarray, list)) else list(embedding),\n",
    "                \"text\": text\n",
    "            }\n",
    "            f.write(json.dumps(record) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1df83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdeb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_to_jsonl(glove_embedded_pdfs, \"glove_embedded_pdfs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e87154",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_to_jsonl(sbert_embedded_pdfs, \"sbert_embedded_pdfs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_to_jsonl(openai_embedded_pdfs, \"openai_embedded_pdfs.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72e91c",
   "metadata": {},
   "source": [
    "# Retrieve saved data from Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3217f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_embeddings_from_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "glove_embedded_pdfs = load_embeddings_from_jsonl(\"./glove_embedded_pdfs.jsonl\")\n",
    "sbert_embedded_pdfs = load_embeddings_from_jsonl(\"./sbert_embedded_pdfs.jsonl\")\n",
    "openai_embedded_pdfs = load_embeddings_from_jsonl(\"./openai_embedded_pdfs.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec32827",
   "metadata": {},
   "source": [
    "### 4.0 Set Up Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d8195e",
   "metadata": {},
   "source": [
    "* Command to type in terminal: ./qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b12f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"glove_embedding_collection\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c19fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppose you have:\n",
    "# glove_embedded_pdfs = [vector1, vector2, vector3, ...]\n",
    "# texts = [\"Text for doc 1\", \"Text for doc 2\", \"Text for doc 3\", ...]\n",
    "\n",
    "# batch_size = 64\n",
    "\n",
    "# for i in range(0, len(glove_embedded_pdfs), batch_size):\n",
    "#     batch = glove_embedded_pdfs[i:i+batch_size]\n",
    "    \n",
    "#     points = [\n",
    "#         PointStruct(\n",
    "#             id=i+j,\n",
    "#             vector=embedding,\n",
    "#             payload={\"filename\": filename}  # you can also store the filename if you want\n",
    "#         )\n",
    "#         for j, (filename, embedding) in enumerate(batch)\n",
    "#     ]\n",
    "\n",
    "#     client.upsert(\n",
    "#         collection_name=\"glove_embedding_collection\",\n",
    "#         points=points\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd65743",
   "metadata": {},
   "source": [
    "# 4.0 and 5.0 Set Up Vector Database and Index Enbeddings\n",
    "\n",
    "Store the embeddings from each model in separate collections or with different\n",
    "naming conventions.\n",
    "\n",
    "## remember .qdrant to make the program run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5643d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "def upsert_to_qdrant(embedded_data, model_name, vector_size):\n",
    "    collection_name = f\"{model_name}_embedding_collection\"\n",
    "\n",
    "    # Create collection (run only once)\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=vector_size,\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "    batch_size = 64\n",
    "    for i in range(0, len(embedded_data), batch_size):\n",
    "        batch = embedded_data[i:i+batch_size]\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=i + j,\n",
    "                vector=embedding.tolist() if hasattr(embedding, \"tolist\") else list(embedding),\n",
    "                payload={\n",
    "                    \"filename\": filename,\n",
    "                    \"text\": text\n",
    "                }\n",
    "            )\n",
    "            for j, (filename, embedding, text) in enumerate(batch)\n",
    "        ]\n",
    "\n",
    "        client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "    print(f\"Upserted {len(embedded_data)} vectors to {collection_name}.\")\n",
    "\n",
    "def upsert_to_qdrant_AI(embedded_data, model_name, vector_size):\n",
    "    collection_name = f\"{model_name}_embedding_collection\"\n",
    "\n",
    "    # Create collection (run only once)\n",
    "    client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=vector_size,\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "    batch_size = 64\n",
    "    for i in range(0, len(embedded_data), batch_size):\n",
    "        batch = embedded_data[i:i+batch_size]\n",
    "\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=i + j,\n",
    "                vector=entry[\"embedding\"],\n",
    "                payload={\n",
    "                    \"filename\": entry[\"filename\"],\n",
    "                    \"text\": entry[\"text\"]\n",
    "                }\n",
    "            )\n",
    "            for j, entry in enumerate(batch)\n",
    "        ]\n",
    "\n",
    "        client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "    print(f\"Upserted {len(embedded_data)} vectors to {collection_name}.\")\n",
    "\n",
    "# RAG Search\n",
    "def search_qdrant(query_text, model_name, vector_size, limit=3):\n",
    "    collection_name = f\"{model_name}_embedding_collection\"\n",
    "    \n",
    "    if model_name == \"glove\":\n",
    "        query_vector = glove_embed(query_text, glove_embeddings, vector_size=vector_size)\n",
    "    elif model_name == \"sbert\":\n",
    "        query_vector = sbert_embed(query_text)\n",
    "    elif model_name == \"open_ai\":\n",
    "        query_vector = openai_embed(query_text)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=limit\n",
    "    )\n",
    "\n",
    "    docs = []\n",
    "    for hit in results:\n",
    "        payload = hit.payload\n",
    "        if 'text' in payload:\n",
    "            docs.append(payload['text'])\n",
    "        else:\n",
    "            print(\"Missing 'text' in payload:\", payload)\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "288f53a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78.1\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(openai.__version__)  # MUST show 0.28.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd60c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "\n",
    "# sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "pdfs = load_pdfs_from_folder(\"./data/v1/docs\")\n",
    "# sbert_embedded_pdfs = []\n",
    "\n",
    "# for filename, text in pdfs:  # Assuming `pdfs` = List[(filename, full_text)]\n",
    "#     for chunk in chunk_text(text, max_tokens=500):  # Define your chunk_text() earlier\n",
    "#         embedding = sbert_model.encode(chunk)\n",
    "#         sbert_embedded_pdfs.append((filename, embedding, chunk))\n",
    "\n",
    "# upsert_to_qdrant(sbert_embedded_pdfs, model_name=\"sbert\", vector_size=384)\n",
    "\n",
    "embedded_openai = []\n",
    "\n",
    "for filename, text in pdfs:\n",
    "    for chunk in chunk_text(text, max_tokens=500):\n",
    "        embedding = openai_embed(chunk)\n",
    "        embedded_openai.append((filename, embedding, chunk))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b763cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'data/v1/docs/2014-annual-performance-report.pdf', 'embedding': [-0.0009930044179782271, 0.03040623851120472, -0.05109354108572006, -0.017868496477603912, -0.05982931703329086, 0.03523042052984238, -0.009483269415795803, 0.03301677107810974, -0.06632290780544281, 0.04859337583184242, 0.008469829335808754, 0.02169446460902691, 0.03288279473781586, 0.013537784107029438, -0.035120826214551926, 0.07195242494344711, 0.08960121124982834, -0.006811867468059063, -0.048532407730817795, -0.016011826694011688, 0.042757805436849594, -0.009645757265388966, 0.04629439488053322, 0.0009178844629786909, -0.06630199402570724, 0.018715113401412964, -0.015667200088500977, -0.024834593757987022, -0.07042499631643295, -0.05278418958187103, -0.03252672404050827, 0.09703652560710907, 0.059942156076431274, 0.030862312763929367, 0.025270432233810425, 0.11491391807794571, 0.05684321001172066, -0.012333068065345287, 0.0303468219935894, -0.0031784474849700928, -0.06252648681402206, -0.10856550931930542, -0.011105349287390709, -0.06433594971895218, -0.0003040259762201458, -0.07007905840873718, -0.05443352460861206, -0.09348832815885544, -0.006196784321218729, -0.025034576654434204, -0.15739686787128448, -0.03236440196633339, -0.053422633558511734, -0.011519542895257473, -0.01849822886288166, -0.011056220158934593, -0.05230602249503136, -0.10695371776819229, -0.050729185342788696, -0.05855624005198479, -0.074710913002491, 0.03489290177822113, -0.0649339109659195, 0.03634681925177574, -0.0012883937451988459, 0.05430315062403679, -0.030897999182343483, -0.13781341910362244, -0.009100607596337795, -0.04553060978651047, -0.007436527404934168, -0.05829649046063423, 0.004147053696215153, -0.09274447709321976, -0.05996446684002876, 0.010338211432099342, 0.012705981731414795, 0.003344521624967456, 0.047544706612825394, -0.140229269862175, 0.10033398866653442, -0.015417253598570824, -0.03571946918964386, 0.06923185288906097, -0.011592729948461056, -0.034301869571208954, -0.03724575415253639, 0.012943429872393608, -0.035089995712041855, 0.010210828855633736, 0.009036155417561531, -0.012246726080775261, -0.036244213581085205, -0.01668977551162243, -0.05479734390974045, -0.03788858279585838, -0.09862546622753143, -0.02555432915687561, 0.007447750307619572, 0.054942354559898376, 0.05706358328461647, 0.00046116820885799825, -0.04847459867596626, -0.04558416083455086, -0.11451559513807297, -0.013719496317207813, 0.10553805530071259, 0.04260088875889778, -0.05147600546479225, 0.014637249521911144, 0.04919413477182388, 0.010500195436179638, -0.10678835213184357, -0.1267043650150299, -0.01496178936213255, 0.012449369765818119, -0.14430226385593414, 0.03857669234275818, 0.022884175181388855, 0.03047041967511177, 0.04303252696990967, -0.033451225608587265, 0.03598273545503616, 0.03847704082727432, -0.01491872500628233, -0.031168542802333832, 0.015611372888088226, -1.0032534982317164e-32, -0.06760904937982559, -0.02445095404982567, 0.030383383855223656, 0.03530413657426834, -0.0011041947873309255, 0.07749735563993454, -0.00469729071483016, 0.010907760821282864, -0.011687638238072395, -0.0253659226000309, -0.01892731711268425, 0.014333343133330345, 0.03848104923963547, 0.007284402847290039, 0.03355858847498894, -0.11812160164117813, 0.019226929172873497, 0.14237834513187408, -0.0986112579703331, 0.004030649550259113, 0.055639006197452545, 0.010981244966387749, -0.06465478986501694, 0.02407858520746231, 0.1624102145433426, 0.10012566298246384, -0.025391502305865288, -0.018680095672607422, -0.0752771869301796, 0.013978089205920696, 0.02570778876543045, 0.14883409440517426, 0.04885942488908768, -0.012841551564633846, -0.011001629754900932, 0.018804162740707397, -0.03415410593152046, -0.004506959114223719, 0.0324600450694561, -0.01680552028119564, -0.08083677291870117, 0.04130181297659874, 0.010646606795489788, 0.028201868757605553, -0.025220178067684174, 0.004323569592088461, -0.013207405805587769, -0.025711463764309883, 0.04216199740767479, 0.051520127803087234, -0.026681063696742058, 0.03001493215560913, -0.02200949937105179, -0.04478735476732254, 0.048079174011945724, 0.02641630731523037, 0.01749044470489025, 0.009880687110126019, 0.011118707247078419, 0.032926324754953384, 0.0019110471475869417, 0.008010691963136196, -0.07209710776805878, -0.03163496032357216, -0.009866978973150253, -0.009819632396101952, -0.05421295017004013, -0.034739408642053604, 0.02746421843767166, 0.056287121027708054, -0.08976354449987411, -0.009486855939030647, 0.02744980901479721, 0.013044379651546478, -0.014259200543165207, -0.011224450543522835, 0.06543146073818207, -0.0007723603048361838, -0.04804730787873268, 0.005478099919855595, -0.061794720590114594, -0.011763935908675194, 0.025347735732793808, -0.062346331775188446, 0.020501451566815376, 0.05088596045970917, 0.06427198648452759, -0.04443233087658882, -0.07673368602991104, 0.05342663452029228, 0.012124977074563503, -0.011799444444477558, -0.08420690149068832, 0.08924180269241333, 0.013886846601963043, -1.9700620430789355e-33, 0.0029418524354696274, 0.05214923247694969, -0.036053214222192764, -0.026456715539097786, 0.012738457880914211, -0.01423060055822134, -0.009129995480179787, 0.013430455699563026, -0.07281453162431717, 0.016274625435471535, -0.0008535741362720728, 0.018159491941332817, -0.007676088251173496, 0.026242023333907127, -0.07284250110387802, 0.019442403689026833, 0.05269293859601021, -0.03240589052438736, -0.06177638843655586, 0.0037087472155690193, 0.04940851777791977, 0.11921359598636627, -0.04025246202945709, 0.019072644412517548, -0.01213000901043415, 0.03681613877415657, 0.021649274975061417, -0.09092050790786743, -0.04370458424091339, -0.08881288021802902, -0.05837671458721161, -0.06828755885362625, -0.057375840842723846, 0.03205566480755806, -0.027808373793959618, 0.0016621779650449753, 0.04584268480539322, -0.046167805790901184, -0.05079967528581619, 0.08256228268146515, 0.06641776859760284, 0.01965845748782158, -0.019651994109153748, 0.04217934608459473, -0.09022306650876999, -0.04067905247211456, 0.0412047877907753, -0.0030221757479012012, -0.06031029298901558, -0.04348396882414818, -0.05286029353737831, -0.04102974012494087, -0.1669687032699585, -0.04470976069569588, -0.04778138920664787, -0.009678700938820839, -0.04798024147748947, -0.09063580632209778, -0.04118882492184639, -0.0474323071539402, 0.08625193685293198, 0.04339629411697388, -0.05856231600046158, -0.0007786390488035977, 0.005088615696877241, -0.008466312661767006, 0.055087246000766754, -0.013714899308979511, -0.052447207272052765, 0.0826345682144165, -0.024762727320194244, -0.0725269615650177, -0.049687888473272324, -0.020610537379980087, 0.039501432329416275, 0.015085102058947086, -0.06835462152957916, 0.0008532767533324659, -0.046889789402484894, 0.044861383736133575, -0.0017205788753926754, -0.01767236553132534, -0.06975693255662918, 0.04659339413046837, -0.04094792902469635, 0.024078700691461563, 0.06679216772317886, -0.0465952530503273, -0.06932403892278671, 0.012586107477545738, -0.06428790092468262, -0.013267584145069122, -0.06544392555952072, 0.011458010412752628, 0.03247182071208954, -5.1547488055803115e-08, -0.013190465979278088, -0.03360985219478607, 0.0194181427359581, -0.01355087198317051, 0.029484426602721214, -0.05609209090471268, -0.026083296164870262, 0.023193778470158577, -0.030195383355021477, -0.006837129592895508, 0.04569479078054428, -0.06288453936576843, -0.09372326731681824, -0.002202240750193596, 0.04769423231482506, -0.07897083461284637, -0.01596035808324814, 0.05462856590747833, -0.04987972602248192, -0.06603805720806122, -0.03151034191250801, -0.021404890343546867, -0.013591882772743702, -0.026237189769744873, -0.016416538506746292, 0.0014497743686661124, -0.07349336892366409, 0.029228847473859787, -0.03552118316292763, 0.048478905111551285, 0.05249675363302231, 0.08262893557548523, -0.006910341791808605, -0.04547901451587677, 0.0309770405292511, -0.040260955691337585, 0.018147900700569153, 0.029368963092565536, -0.016012605279684067, 0.0068411072716116905, -0.018079673871397972, -0.047367967665195465, 0.011112257838249207, 0.10780667513608932, 0.002391883870586753, 0.03703271597623825, -0.05919070541858673, -0.11115124821662903, -0.030007407069206238, -0.07544810324907303, 0.023055564612150192, -0.03115331381559372, -0.028759734705090523, 0.018382705748081207, 0.003602369921281934, 0.06946118921041489, 0.09174976497888565, -0.03919031843543053, -0.07266800105571747, -0.011728592216968536, 0.027905970811843872, -0.0065586622804403305, -0.04035240784287453, 0.024656690657138824], 'text': 'United States Office of Personnel Management Annual Performance Report Fiscal Year 2014 United States Office of Personnel Management February 2015The United States Office of Personnel Management Fiscal Year 2014 Annual Performance ReportOPM Fiscal Year 2014 Annual Performance ReportvTable of Contents Message from the Director . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5 About this Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6 OPM’s Mission, Vision, Values, and Strategic Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .7 FY 2014 Organizational Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .9 FY 2014 Major Management Priorities, Challenges and Risks . . . . . . . . . . . . . . . . . . . . . . . . .13 Cross-Agency Priority Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15 Agency Priority Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16 FY 2014 Performance Results . . . . . . . . . . .'}\n"
     ]
    }
   ],
   "source": [
    "print(openai_embedded_pdfs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86dad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedded_openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_to_jsonl(embedded_openai, \"openai_embedded_pdfs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de970034",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_to_qdrant_AI(openai_embedded_pdfs, model_name=\"openai\", vector_size=1536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db87e1",
   "metadata": {},
   "source": [
    "# 6.0 Implement RAG\n",
    "Create a simple RAG pipeline using each set of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69aff91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(query, embed_fn, collection_name, llm_fn, top_k=5, **embed_args):\n",
    "    from qdrant_client import QdrantClient\n",
    "\n",
    "    # Connect to Qdrant\n",
    "    client = QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    # Embed query\n",
    "    query_vec = embed_fn(query, **embed_args)\n",
    "\n",
    "    # Search Qdrant\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vec,\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    # Extract top-k text chunks\n",
    "    chunks = [hit.payload['text'] for hit in search_result]\n",
    "    context = \"\\n\\n\".join(chunks)\n",
    "\n",
    "    # Create prompt for LLM\n",
    "    prompt = f\"Use the context to answer:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    # Generate answer using LLM\n",
    "    return llm_fn(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de600c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def openai_generate(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=300,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16666ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m collections = \u001b[43mclient\u001b[49m.get_collections()\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(collections)\n",
      "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "collections = client.get_collections()\n",
    "print(collections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = client.search(\n",
    "    collection_name=f\"{embed_type}_collection\",\n",
    "    query_vector=query_embedding,\n",
    "    limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98461990",
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while deserializing header: HeaderTooLarge",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat was Microsoft\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms net cash from operating activities in Q3 2022?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# glove_answer = simple_rag(query, glove_embed, \"glove_embedding_collection\", openai_generate, glove_embeddings=glove_embeddings)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# print(\"Glove RAG Answer:\\n\", glove_answer)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sbert_answer = \u001b[43msimple_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msbert_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msbert_embedding_collection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai_generate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSBERT RAG Answer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, sbert_answer)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# openai_answer = simple_rag(query, openai_embed, \"openai_embedding_collection\", openai_generate)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# print(\"OpenAI RAG Answer:\\n\", openai_answer)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36msimple_rag\u001b[39m\u001b[34m(query, embed_fn, collection_name, llm_fn, top_k, **embed_args)\u001b[39m\n\u001b[32m      5\u001b[39m client = QdrantClient(host=\u001b[33m\"\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m\"\u001b[39m, port=\u001b[32m6333\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Embed query\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m query_vec = \u001b[43membed_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43membed_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Search Qdrant\u001b[39;00m\n\u001b[32m     11\u001b[39m search_result = client.search(\n\u001b[32m     12\u001b[39m     collection_name=collection_name,\n\u001b[32m     13\u001b[39m     query_vector=query_vec,\n\u001b[32m     14\u001b[39m     limit=top_k\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36msbert_embed\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msbert_embed\u001b[39m(text):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.encode(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/krag_project/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:309\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    300\u001b[39m         model_name_or_path = __MODEL_HUB_ORGANIZATION__ + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m + model_name_or_path\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[32m    303\u001b[39m     model_name_or_path,\n\u001b[32m    304\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    308\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    321\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    322\u001b[39m         model_name_or_path,\n\u001b[32m    323\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         config_kwargs=config_kwargs,\n\u001b[32m    331\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/krag_project/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:1808\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   1805\u001b[39m \u001b[38;5;66;03m# Try to initialize the module with a lot of kwargs, but only if the module supports them\u001b[39;00m\n\u001b[32m   1806\u001b[39m \u001b[38;5;66;03m# Otherwise we fall back to the load method\u001b[39;00m\n\u001b[32m   1807\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1808\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1810\u001b[39m     module = module_class.load(model_name_or_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/krag_project/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:81\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     78\u001b[39m     config_args = {}\n\u001b[32m     80\u001b[39m config, is_peft_model = \u001b[38;5;28mself\u001b[39m._load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_peft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[32m     84\u001b[39m     tokenizer_args[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m] = max_seq_length\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/krag_project/lib/python3.11/site-packages/sentence_transformers/models/Transformer.py:181\u001b[39m, in \u001b[36mTransformer._load_model\u001b[39m\u001b[34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_mt5_model(model_name_or_path, config, cache_dir, **model_args)\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28mself\u001b[39m.auto_model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft_model:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_peft_model(model_name_or_path, config, cache_dir, **model_args, **adapter_only_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/krag_project/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/krag_project/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/krag_project/lib/python3.11/site-packages/transformers/modeling_utils.py:4288\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4280\u001b[39m is_from_file = pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4283\u001b[39m     is_safetensors_available()\n\u001b[32m   4284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_from_file\n\u001b[32m   4285\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded\n\u001b[32m   4286\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m checkpoint_files[\u001b[32m0\u001b[39m].endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4287\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m4288\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   4289\u001b[39m         metadata = f.metadata()\n\u001b[32m   4291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4292\u001b[39m         \u001b[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while deserializing header: HeaderTooLarge"
     ]
    }
   ],
   "source": [
    "query = \"What was Microsoft's net cash from operating activities in Q3 2022?\"\n",
    "\n",
    "# glove_answer = simple_rag(query, glove_embed, \"glove_embedding_collection\", openai_generate, glove_embeddings=glove_embeddings)\n",
    "# print(\"Glove RAG Answer:\\n\", glove_answer)\n",
    "\n",
    "sbert_answer = simple_rag(query, sbert_embed, \"sbert_embedding_collection\", openai_generate)\n",
    "print(\"SBERT RAG Answer:\\n\", sbert_answer)\n",
    "\n",
    "# openai_answer = simple_rag(query, openai_embed, \"openai_embedding_collection\", openai_generate)\n",
    "# print(\"OpenAI RAG Answer:\\n\", openai_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ac5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = \"23.4 billion\"\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def compare(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "print(\"🔍 SBERT score:\", compare(sbert_answer, expected))\n",
    "print(\"🔍 OpenAI score:\", compare(openai_answer, expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embedding = embed_query(\"What is photosynthesis?\")  # Get embedding of query\n",
    "\n",
    "# search_result = client.search(\n",
    "#     collection_name=\"glove_embedding_collection\",\n",
    "#     query_vector=query_embedding,\n",
    "#     limit=3\n",
    "# )\n",
    "\n",
    "# # Extract relevant documents\n",
    "# docs = [hit.payload['text'] for hit in search_result]\n",
    "\n",
    "# # Combine docs + user query to create a context-rich prompt for LLM\n",
    "# rag_prompt = \"Use the following documents to answer:\\n\\n\" + \"\\n\\n\".join(docs) + \"\\n\\nQuestion: What is photosynthesis?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cdfef",
   "metadata": {},
   "source": [
    "# 7.0 Evaluate Performance\n",
    "\n",
    "Test the RAG system with a set of queries and evaluate based on metrics like\n",
    "- relevance, \n",
    "- accuracy, and \n",
    "- response quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04ae10",
   "metadata": {},
   "source": [
    "This project ended up not working, so I couldn't evaluate anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8ee5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e061a86dcb54eeead8ccb6a2b059b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b389a294b4b94451b076b1fee4a40493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4060e6c34dcb4b9593f0dceec0427b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f78136cd79e479e9f72236619138ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957d46de405643178891cb236b7a5611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/827 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d100e6f5c5b4f1a897ca4953a972706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cfe792ef614e3e8288c80fc508d7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398f5ff5a1c7466290ed32e68612b84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39a305d9c6d4c8eaa52437286117a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a923b12dd97843f5a6f30741dcd5ad5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e7abb91a544766ad1183bcee693b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 768)\n",
      "tensor([[1.0000, 0.5926, 0.1182],\n",
      "        [0.5926, 1.0000, 0.1695],\n",
      "        [0.1182, 0.1695, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"paraphrase-albert-small-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd5290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krag_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
